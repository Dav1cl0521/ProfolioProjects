{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f2010cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.common.exceptions import StaleElementReferenceException, TimeoutException\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from urllib.parse import urljoin\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d83902d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data exported to data.csv\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.common.exceptions import StaleElementReferenceException, TimeoutException\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from urllib.parse import urljoin\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "# Define the URL\n",
    "url = 'https://www.tsinghua.edu.cn/news/spqh/jctj.htm'\n",
    "\n",
    "driver = webdriver.Safari()\n",
    "\n",
    "# Open the web page in the driver\n",
    "driver.get(url)\n",
    "\n",
    "\n",
    "# Extract the HTML from the web page\n",
    "html = driver.page_source\n",
    "\n",
    "# Parse the HTML with Beautiful Soup\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# Define a list to store the extracted data\n",
    "df = pd.DataFrame(columns=['Title', 'Date', 'URL', 'Description'])\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        # Extract the HTML from the web page\n",
    "        html = driver.page_source\n",
    "\n",
    "        # Parse the HTML with Beautiful Soup\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "        # Extract the data from the soup object\n",
    "        video_title = soup.find_all('p', class_=\"bt\")[1:9]\n",
    "        titles = [vedio.text.strip() for vedio in video_title]\n",
    "\n",
    "        # Find the <ul> tag with class \"qhrw2_ul\"\n",
    "        ul_tag = soup.find('ul', class_='qhrw2_ul')\n",
    "        # Find all <a> tags within the <ul> tag and get their href attributes     \n",
    "        links = [urljoin(url, a_tag.get('href')) for a_tag in ul_tag.find_all('a')]\n",
    "\n",
    "        # Find all <div> tags with class \"sj\"\n",
    "        div_tags = ul_tag.find_all('div', class_='sj')\n",
    "        # Loop through each <div> tag and get the date\n",
    "        dates = []\n",
    "        for div_tag in div_tags:\n",
    "            p_tag = div_tag.find('p')\n",
    "            span_tag = div_tag.find('span')\n",
    "            date_str = f\"{p_tag.text}/{span_tag.text}\"\n",
    "            date = datetime.strptime(date_str, '%d/%Y.%m')\n",
    "            dates.append(date.strftime('%Y/%m/%d'))\n",
    "\n",
    "        # For the description    \n",
    "        words = soup.find_all('p', class_='zy')[1:]\n",
    "        word = [word.text.strip() for word in words]\n",
    "\n",
    "        # Ensure that all the lists in the dictionary have the same length\n",
    "        num_rows = min(len(titles), len(dates), len(links), len(word))\n",
    "        new_data = {'Title': titles[:num_rows], 'Date': dates[:num_rows], 'URL': links[:num_rows], 'Description': word[:num_rows]}\n",
    "\n",
    "        # Convert the dictionary to a Pandas DataFrame\n",
    "        new_df = pd.DataFrame(new_data)\n",
    "\n",
    "        # Append the new DataFrame to the original DataFrame\n",
    "        df = pd.concat([df, new_df], ignore_index=True)\n",
    "\n",
    "        # Wait for the next button to become clickable\n",
    "        wait = WebDriverWait(driver, 10)\n",
    "        next_link = wait.until(EC.element_to_be_clickable((By.XPATH, \"//a[@class='next']\")))\n",
    "\n",
    "        # Check if the next link has a valid href attribute\n",
    "        if next_link.get_attribute('href') == \"javascript:;\":\n",
    "            # If the href attribute is \"javascript:;\", it means that we have reached the last page\n",
    "            break\n",
    "        else:\n",
    "            # Click on the next button\n",
    "            next_link.click()\n",
    "\n",
    "    except StaleElementReferenceException:\n",
    "        # If the element reference is stale, locate the element again\n",
    "        pass\n",
    "\n",
    "    except TimeoutException:\n",
    "        # If the \"next\" button is not clickable, it means that we have reached the last page\n",
    "        break\n",
    "\n",
    "\n",
    "\n",
    "# Extract data from the first featured post\n",
    "video_title = soup.find_all('p', class_=\"bt\")[0]\n",
    "titles = [vedio.text.strip() for vedio in video_title]\n",
    "\n",
    "ul_tag = soup.find('div', class_='qhrw2_first')\n",
    "links = [urljoin(url, a_tag.get('href')) for a_tag in ul_tag.find_all('a')]\n",
    "\n",
    "ul_tag = soup.find('div', class_='sj')\n",
    "dates = []\n",
    "p_tag = ul_tag.find('p')\n",
    "span_tag = ul_tag.find('span')\n",
    "date_str = f\"{p_tag.text}/{span_tag.text}\"\n",
    "date = datetime.strptime(date_str, '%d/%Y.%m')\n",
    "dates.append(date.strftime('%Y/%m/%d'))\n",
    "\n",
    "words = soup.find_all('p', class_='zy')[0]\n",
    "word = [word.text.strip() for word in words]\n",
    "\n",
    "new_data = {'Title': titles, 'Date': dates, 'URL': links, 'Description': word}\n",
    "new_df = pd.DataFrame(new_data)\n",
    "df = pd.concat([df, new_df], ignore_index=True)\n",
    "\n",
    "# Close the driver\n",
    "driver.quit()\n",
    "\n",
    "df.drop_duplicates(inplace=True)\n",
    "\n",
    "# Export the DataFrame to a CSV file\n",
    "df.to_csv(r'/Users/davidsmacbook/Desktop/Work/Data Scraping from THU.csv', index=False)\n",
    "\n",
    "# Print a message to confirm the export\n",
    "print('Data exported to data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18b4523d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'driver' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m driver\u001b[38;5;241m.\u001b[39mquit()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'driver' is not defined"
     ]
    }
   ],
   "source": [
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "26b762cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['【2023毕业故事】李润凤：去支教 让梦想在西部绽放']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First Featured post\n",
    "video_title = soup.find_all('p', class_ =\"bt\")[0]\n",
    "titles = [vedio.text.strip() for vedio in video_title]\n",
    "titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7c393e54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../../info/2061/105129.htm']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the <ul> tag with class \"qhrw2_first\"\n",
    "\n",
    "ul_tag = soup.find('div', class_='qhrw2_first')\n",
    "links = [a_tag.get('href') for a_tag in ul_tag.find_all('a')]\n",
    "\n",
    "links\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "99905e6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2023/06/30']"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the <ul> tag with class \"qhrw2_ul\"\n",
    "ul_tag = soup.find('div', class_='sj')\n",
    "dates = []\n",
    "p_tag = ul_tag.find('p')\n",
    "span_tag = ul_tag.find('span')\n",
    "date_str = f\"{p_tag.text}/{span_tag.text}\"\n",
    "date = datetime.strptime(date_str, '%d/%Y.%m')\n",
    "dates.append(date.strftime('%Y/%m/%d'))\n",
    "\n",
    "\n",
    "dates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c61c15ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['在清华的4年，李润凤用攀登这个关键词来概括自己的成长，不断地去爬台阶，从0走向了无限可能。如今完成本科学业的她，毕业后将前往云南省南涧彝族自治县支教，到祖国和人民需要的地方去，为实现高质量发展，为实现中华民族伟大复兴去贡献一份青春力量。']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = soup.find_all('p', class_='zy')[0]\n",
    "\n",
    "word = [word.text.strip() for word in words]\n",
    "\n",
    "word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c6a790b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['【2023毕业故事】投身基层服务！清华“励志学子”林万东毕业回家乡',\n",
       " '【2023毕业故事】代宗仁：我和家乡有个约定',\n",
       " '【2023毕业故事】去祖国西部当老师！轮椅上的清华博士毕业了',\n",
       " '【2023毕业故事】潘长城：投身国家水安全保障事业',\n",
       " '加入为先书院，成为定义未来的科技领导者！',\n",
       " '【秀钟书院招生宣传片】水木清华众秀钟',\n",
       " '校党委书记邱勇一行来到清华附小，与师生共庆“六一”',\n",
       " '学习贯彻习近平新时代中国特色社会主义思想主题教育专题研修主题报告会']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_title = soup.find_all('p', class_ =\"bt\")[1:9]\n",
    "titles = [vedio.text.strip() for vedio in video_title]\n",
    "\n",
    "titles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b0df7271",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../../info/2061/105058.htm',\n",
       " '../../info/2061/104987.htm',\n",
       " '../../info/2061/104962.htm',\n",
       " '../../info/2061/104958.htm',\n",
       " '../../info/2061/104884.htm',\n",
       " '../../info/2061/104882.htm',\n",
       " '../../info/2061/104705.htm',\n",
       " '../../info/2061/104022.htm']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the <ul> tag with class \"qhrw2_ul\"\n",
    "ul_tag = soup.find('ul', class_='qhrw2_ul')\n",
    "\n",
    "# Find all <a> tags within the <ul> tag and get their href attributes\n",
    "links = [a_tag.get('href') for a_tag in ul_tag.find_all('a')]\n",
    "\n",
    "#links = [link.get('herf') for link in soup.find_all('ul', class_='qhrw2_ul')]\n",
    "\n",
    "links\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "9654bcff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2023/06/30', '2023/06/27', '2023/06/25', '2023/06/24', '2023/06/24', '2023/06/20', '2023/06/20', '2023/06/02', '2023/05/21']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Find the <ul> tag with class \"qhrw2_ul\"\n",
    "ul_tag = soup.find('ul', class_='qhrw2_ul')\n",
    "\n",
    "# Find all <div> tags with class \"sj\"\n",
    "div_tags = ul_tag.find_all('div', class_='sj')\n",
    "\n",
    "# Loop through each <div> tag and get the date\n",
    "dates = []\n",
    "for div_tag in div_tags:\n",
    "    p_tag = div_tag.find('p')\n",
    "    span_tag = div_tag.find('span')\n",
    "    date_str = f\"{p_tag.text}/{span_tag.text}\"\n",
    "    date = datetime.strptime(date_str, '%d/%Y.%m')\n",
    "    dates.append(date.strftime('%Y/%m/%d'))\n",
    "\n",
    "# Print the list of dates\n",
    "print(dates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "02d396ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['四年前，正在工地搬砖的云南考生林万东收到了来自清华大学的录取通知书。如今，顺利完成本科学业的他，选择回到家乡投身基层公共部门工作。他说，“希望自己化作一颗小小的‘炭火’，无声燃烧，却可以传递温暖。”万东，加油！',\n",
       " '“回家乡去！投身国防军工事业！”当毕业去哪这一重要抉择来临之际，清华大学精仪系博士研究生代宗仁坚定地给出了他的答案。他希望自己向下扎根、向上成长，“做一名快乐生活、阳光热血的新青年，把自己的一生写进祖国的建设历史中。”',\n",
       " '17岁时，他以优异的成绩考入清华大学，曾经的运动达人却在大一下学期突发脊髓血管瘤全身瘫痪。清华“自强不息”的精神激励着他一点点重拾信心，坐上轮椅回到校园后，他以惊人的毅力顺利完成了本科、博士阶段的学业，并选择去祖国西部从事教书育人和科研工...',\n",
       " '又到了凤凰花开的季节，毕业季如期而至。从校门到社会，清华园里莘莘学子即将奔赴人生的下一站。当“毕业去哪儿”这一重要抉择来临之际，清华大学水利水电工程系博士研究生潘长城，坚定地给出了他的答案：投身国家水安全保障事业，践行清华人的使命与担当...',\n",
       " '为先书院作为清华大学创新工科的先行示范，探索科技创新领军人才培养的新模式。 为先书院以为国家培养未来科技创新领军人才为使命任务，构筑“宽厚基础-工程实践-探索研究”三要素融合的培养体系、“师生从游、滴灌培育”的教学模式。为先书院注重学生过程...',\n",
       " '为满足绿色低碳发展战略需求，破解全球可持续发展重大命题，清华大学成立秀钟书院，并于今年开始招生。如果你心怀地球与人类命运，学术志趣浓厚，创新潜力突出，欢迎加入秀钟书院！',\n",
       " '在“六一”国际儿童节到来之际，校党委书记邱勇一行来到清华附小，与师生共庆“六一”，向大家致以节日祝贺，向辛勤工作的附小教师表示诚挚问候。',\n",
       " '在主题教育如火如荼开展之时，5月20日上午，新清华学堂再掀热潮。作为清华大学学习贯彻习近平新时代中国特色社会主义思想主题教育读书班特色活动之一，校党委书记邱勇以“为以中国式现代化全面推进中华民族伟大复兴贡献清华力量”为题作专题研修主题报告。']"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = soup.find_all('p', class_='zy')[1:]\n",
    "\n",
    "word = [word.text.strip() for word in words]\n",
    "\n",
    "word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0152c6e",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3143688150.py, line 11)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[5], line 11\u001b[0;36m\u001b[0m\n\u001b[0;31m    Words =\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Extract the data from the soup object\n",
    "video_title = soup.find_all('div', class_ =\"item-title04 item-title05\")\n",
    "titles = [vedio.text.strip() for vedio in video_title]\n",
    "\n",
    "dates = soup.find_all('span', class_ =\"item-date\")[3:]\n",
    "date = [vedio.text.strip() for vedio in dates]\n",
    "\n",
    "links = soup.find_all('source')\n",
    "link = [vediolink['src'] for vediolink in links][3:]\n",
    "\n",
    "Words =\n",
    "word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6c3b569",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad96454f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e595d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
