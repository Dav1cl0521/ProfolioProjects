{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "627b2bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.common.exceptions import StaleElementReferenceException, TimeoutException\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from urllib.parse import urljoin\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a97e8a0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# This is the first page \"精彩推荐\"\n",
    "\n",
    "\n",
    "# Define the URL\n",
    "url = 'https://www.tsinghua.edu.cn/news/spqh/jctj.htm'\n",
    "\n",
    "driver = webdriver.Safari()\n",
    "\n",
    "# Open the web page in the driver\n",
    "driver.get(url)\n",
    "\n",
    "\n",
    "# Extract the HTML from the web page\n",
    "html = driver.page_source\n",
    "\n",
    "# Parse the HTML with Beautiful Soup\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# Define a list to store the extracted data\n",
    "df = pd.DataFrame(columns=['Title', 'Date', 'URL', 'Description'])\n",
    "\n",
    "\n",
    "# Extract data from the first featured post\n",
    "video_title = soup.find_all('p', class_=\"bt\")[0]\n",
    "titles = [vedio.text.strip() for vedio in video_title]\n",
    "\n",
    "ul_tag = soup.find('div', class_='qhrw2_first')\n",
    "links = [urljoin(url, a_tag.get('href')) for a_tag in ul_tag.find_all('a')]\n",
    "\n",
    "ul_tag = soup.find('div', class_='sj')\n",
    "dates = []\n",
    "p_tag = ul_tag.find('p')\n",
    "span_tag = ul_tag.find('span')\n",
    "date_str = f\"{p_tag.text}/{span_tag.text}\"\n",
    "date = datetime.strptime(date_str, '%d/%Y.%m')\n",
    "dates.append(date.strftime('%Y/%m/%d'))\n",
    "\n",
    "words = soup.find_all('p', class_='zy')[0]\n",
    "word = [word.text.strip() for word in words]\n",
    "\n",
    "new_data = {'Title': titles, 'Date': dates, 'URL': links, 'Description': word}\n",
    "new_df = pd.DataFrame(new_data)\n",
    "df = pd.concat([df, new_df], ignore_index=True)\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        # Extract the HTML from the web page\n",
    "        html = driver.page_source\n",
    "\n",
    "        # Parse the HTML with Beautiful Soup\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "        # Extract the data from the soup object\n",
    "        video_title = soup.find_all('p', class_=\"bt\")[1:9]\n",
    "        titles = [vedio.text.strip() for vedio in video_title]\n",
    "\n",
    "        # Find the <ul> tag with class \"qhrw2_ul\"\n",
    "        ul_tag = soup.find('ul', class_='qhrw2_ul')\n",
    "        # Find all <a> tags within the <ul> tag and get their href attributes     \n",
    "        links = [urljoin(url, a_tag.get('href')) for a_tag in ul_tag.find_all('a')]\n",
    "\n",
    "        # Find all <div> tags with class \"sj\"\n",
    "        div_tags = ul_tag.find_all('div', class_='sj')\n",
    "        # Loop through each <div> tag and get the date\n",
    "        dates = []\n",
    "        for div_tag in div_tags:\n",
    "            p_tag = div_tag.find('p')\n",
    "            span_tag = div_tag.find('span')\n",
    "            date_str = f\"{p_tag.text}/{span_tag.text}\"\n",
    "            date = datetime.strptime(date_str, '%d/%Y.%m')\n",
    "            dates.append(date.strftime('%Y/%m/%d'))\n",
    "\n",
    "        # For the description    \n",
    "        words = soup.find_all('p', class_='zy')[1:]\n",
    "        word = [word.text.strip() for word in words]\n",
    "\n",
    "        # Ensure that all the lists in the dictionary have the same length\n",
    "        num_rows = min(len(titles), len(dates), len(links), len(word))\n",
    "        new_data = {'Title': titles[:num_rows], 'Date': dates[:num_rows], 'URL': links[:num_rows], 'Description': word[:num_rows]}\n",
    "\n",
    "        # Convert the dictionary to a Pandas DataFrame\n",
    "        new_df = pd.DataFrame(new_data)\n",
    "\n",
    "        # Append the new DataFrame to the original DataFrame\n",
    "        df = pd.concat([df, new_df], ignore_index=True)\n",
    "\n",
    "        # Wait for the next button to become clickable\n",
    "        wait = WebDriverWait(driver, 10)\n",
    "        next_link = wait.until(EC.element_to_be_clickable((By.XPATH, \"//a[@class='next']\")))\n",
    "\n",
    "        # Check if the next link has a valid href attribute\n",
    "        if next_link.get_attribute('href') == \"javascript:;\":\n",
    "            # If the href attribute is \"javascript:;\", it means that we have reached the last page\n",
    "            break\n",
    "        else:\n",
    "            # Click on the next button\n",
    "            next_link.click()\n",
    "\n",
    "    except StaleElementReferenceException:\n",
    "        # If the element reference is stale, locate the element again\n",
    "        pass\n",
    "\n",
    "    except TimeoutException:\n",
    "        # If the \"next\" button is not clickable, it means that we have reached the last page\n",
    "        break\n",
    "   \n",
    "\n",
    "driver.quit()\n",
    "\n",
    "df.drop_duplicates(inplace=True)\n",
    "\n",
    "df['Category'] = '精彩推荐'\n",
    "\n",
    "df_jctj = df\n",
    "\n",
    "\n",
    "# Print a message to confirm the export\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5a42f3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# # This is the second page \"清华映像\"\n",
    "\n",
    "\n",
    "# Define the URL\n",
    "url = 'https://www.tsinghua.edu.cn/news/spqh/qhyx.htm'\n",
    "\n",
    "driver = webdriver.Safari()\n",
    "\n",
    "# Open the web page in the driver\n",
    "driver.get(url)\n",
    "\n",
    "\n",
    "# Extract the HTML from the web page\n",
    "html = driver.page_source\n",
    "\n",
    "# Parse the HTML with Beautiful Soup\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# Define a list to store the extracted data\n",
    "df = pd.DataFrame(columns=['Title', 'Date', 'URL', 'Description'])\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        # Extract the HTML from the web page\n",
    "        html = driver.page_source\n",
    "\n",
    "        # Parse the HTML with Beautiful Soup\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "        # Extract the data from the soup object\n",
    "        video_title = soup.find_all('p', class_=\"bt\")[1:9]\n",
    "        titles = [vedio.text.strip() for vedio in video_title]\n",
    "\n",
    "        # Find the <ul> tag with class \"qhrw2_ul\"\n",
    "        ul_tag = soup.find('ul', class_='qhrw2_ul')\n",
    "        # Find all <a> tags within the <ul> tag and get their href attributes     \n",
    "        links = [urljoin(url, a_tag.get('href')) for a_tag in ul_tag.find_all('a')]\n",
    "\n",
    "        # Find all <div> tags with class \"sj\"\n",
    "        div_tags = ul_tag.find_all('div', class_='sj')\n",
    "        # Loop through each <div> tag and get the date\n",
    "        dates = []\n",
    "        for div_tag in div_tags:\n",
    "            p_tag = div_tag.find('p')\n",
    "            span_tag = div_tag.find('span')\n",
    "            date_str = f\"{p_tag.text}/{span_tag.text}\"\n",
    "            date = datetime.strptime(date_str, '%d/%Y.%m')\n",
    "            dates.append(date.strftime('%Y/%m/%d'))\n",
    "\n",
    "        # For the description    \n",
    "        words = soup.find_all('p', class_='zy')[1:]\n",
    "        word = [word.text.strip() for word in words]\n",
    "\n",
    "        # Ensure that all the lists in the dictionary have the same length\n",
    "        num_rows = min(len(titles), len(dates), len(links), len(word))\n",
    "        new_data = {'Title': titles[:num_rows], 'Date': dates[:num_rows], 'URL': links[:num_rows], 'Description': word[:num_rows]}\n",
    "\n",
    "        # Convert the dictionary to a Pandas DataFrame\n",
    "        new_df = pd.DataFrame(new_data)\n",
    "\n",
    "        # Append the new DataFrame to the original DataFrame\n",
    "        df = pd.concat([df, new_df], ignore_index=True)\n",
    "\n",
    "        # Wait for the next button to become clickable\n",
    "        wait = WebDriverWait(driver, 10)\n",
    "        next_link = wait.until(EC.element_to_be_clickable((By.XPATH, \"//a[@class='next']\")))\n",
    "\n",
    "        # Check if the next link has a valid href attribute\n",
    "        if next_link.get_attribute('href') == \"javascript:;\":\n",
    "            # If the href attribute is \"javascript:;\", it means that we have reached the last page\n",
    "            break\n",
    "        else:\n",
    "            # Click on the next button\n",
    "            next_link.click()\n",
    "\n",
    "    except StaleElementReferenceException:\n",
    "        # If the element reference is stale, locate the element again\n",
    "        pass\n",
    "\n",
    "    except TimeoutException:\n",
    "        # If the \"next\" button is not clickable, it means that we have reached the last page\n",
    "        break\n",
    "\n",
    "\n",
    "\n",
    "# Close the driver\n",
    "driver.quit()\n",
    "\n",
    "df.drop_duplicates(inplace=True)\n",
    "\n",
    "df['Category'] = '清华映像'\n",
    "\n",
    "df_qhyx = df\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e715229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    " # This is the third page \"专题集锦 \"\n",
    "\n",
    "# Define the URL\n",
    "url = 'https://www.tsinghua.edu.cn/news/spqh/ztjj.htm'\n",
    "\n",
    "driver = webdriver.Safari()\n",
    "\n",
    "# Open the web page in the driver\n",
    "driver.get(url)\n",
    "\n",
    "\n",
    "# Extract the HTML from the web page\n",
    "html = driver.page_source\n",
    "\n",
    "# Parse the HTML with Beautiful Soup\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# Define a list to store the extracted data\n",
    "df = pd.DataFrame(columns=['Title', 'Date', 'URL', 'Description'])\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        # Extract the HTML from the web page\n",
    "        html = driver.page_source\n",
    "\n",
    "        # Parse the HTML with Beautiful Soup\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "        # Extract the data from the soup object\n",
    "        video_title = soup.find_all('p', class_=\"bt\")[1:9]\n",
    "        titles = [vedio.text.strip() for vedio in video_title]\n",
    "\n",
    "        # Find the <ul> tag with class \"qhrw2_ul\"\n",
    "        ul_tag = soup.find('ul', class_='qhrw2_ul')\n",
    "        # Find all <a> tags within the <ul> tag and get their href attributes     \n",
    "        links = [urljoin(url, a_tag.get('href')) for a_tag in ul_tag.find_all('a')]\n",
    "\n",
    "        # Find all <div> tags with class \"sj\"\n",
    "        div_tags = ul_tag.find_all('div', class_='sj')\n",
    "        # Loop through each <div> tag and get the date\n",
    "        dates = []\n",
    "        for div_tag in div_tags:\n",
    "            p_tag = div_tag.find('p')\n",
    "            span_tag = div_tag.find('span')\n",
    "            date_str = f\"{p_tag.text}/{span_tag.text}\"\n",
    "            date = datetime.strptime(date_str, '%d/%Y.%m')\n",
    "            dates.append(date.strftime('%Y/%m/%d'))\n",
    "\n",
    "        # For the description    \n",
    "        words = soup.find_all('p', class_='zy')[1:]\n",
    "        word = [word.text.strip() for word in words]\n",
    "\n",
    "        # Ensure that all the lists in the dictionary have the same length\n",
    "        num_rows = min(len(titles), len(dates), len(links), len(word))\n",
    "        new_data = {'Title': titles[:num_rows], 'Date': dates[:num_rows], 'URL': links[:num_rows], 'Description': word[:num_rows]}\n",
    "\n",
    "        # Convert the dictionary to a Pandas DataFrame\n",
    "        new_df = pd.DataFrame(new_data)\n",
    "\n",
    "        # Append the new DataFrame to the original DataFrame\n",
    "        df = pd.concat([df, new_df], ignore_index=True)\n",
    "\n",
    "        # Wait for the next button to become clickable\n",
    "        wait = WebDriverWait(driver, 10)\n",
    "        next_link = wait.until(EC.element_to_be_clickable((By.XPATH, \"//a[@class='next']\")))\n",
    "\n",
    "        # Check if the next link has a valid href attribute\n",
    "        if next_link.get_attribute('href') == \"javascript:;\":\n",
    "            # If the href attribute is \"javascript:;\", it means that we have reached the last page\n",
    "            break\n",
    "        else:\n",
    "            # Click on the next button\n",
    "            next_link.click()\n",
    "\n",
    "    except StaleElementReferenceException:\n",
    "        # If the element reference is stale, locate the element again\n",
    "        pass\n",
    "\n",
    "    except TimeoutException:\n",
    "        # If the \"next\" button is not clickable, it means that we have reached the last page\n",
    "        break\n",
    "        \n",
    "        \n",
    "\n",
    "# Close the driver\n",
    "driver.quit()\n",
    "\n",
    "df.drop_duplicates(inplace=True)\n",
    "\n",
    "\n",
    "df['Category'] = '专题集锦'\n",
    "\n",
    "df_ztjj = df\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55bd63b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    " # This is the fourth page \"拍客日记 \"\n",
    "        \n",
    "\n",
    "# Define the URL\n",
    "url = 'https://www.tsinghua.edu.cn/news/spqh/pkrj.htm'\n",
    "\n",
    "driver = webdriver.Safari()\n",
    "\n",
    "# Open the web page in the driver\n",
    "driver.get(url)\n",
    "\n",
    "\n",
    "# Extract the HTML from the web page\n",
    "html = driver.page_source\n",
    "\n",
    "# Parse the HTML with Beautiful Soup\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# Define a list to store the extracted data\n",
    "df = pd.DataFrame(columns=['Title', 'Date', 'URL', 'Description'])\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        # Extract the HTML from the web page\n",
    "        html = driver.page_source\n",
    "\n",
    "        # Parse the HTML with Beautiful Soup\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "        # Extract the data from the soup object\n",
    "        video_title = soup.find_all('p', class_=\"bt\")[1:9]\n",
    "        titles = [vedio.text.strip() for vedio in video_title]\n",
    "\n",
    "        # Find the <ul> tag with class \"qhrw2_ul\"\n",
    "        ul_tag = soup.find('ul', class_='qhrw2_ul')\n",
    "        # Find all <a> tags within the <ul> tag and get their href attributes     \n",
    "        links = [urljoin(url, a_tag.get('href')) for a_tag in ul_tag.find_all('a')]\n",
    "\n",
    "        # Find all <div> tags with class \"sj\"\n",
    "        div_tags = ul_tag.find_all('div', class_='sj')\n",
    "        # Loop through each <div> tag and get the date\n",
    "        dates = []\n",
    "        for div_tag in div_tags:\n",
    "            p_tag = div_tag.find('p')\n",
    "            span_tag = div_tag.find('span')\n",
    "            date_str = f\"{p_tag.text}/{span_tag.text}\"\n",
    "            date = datetime.strptime(date_str, '%d/%Y.%m')\n",
    "            dates.append(date.strftime('%Y/%m/%d'))\n",
    "\n",
    "        # For the description    \n",
    "        words = soup.find_all('p', class_='zy')[1:]\n",
    "        word = [word.text.strip() for word in words]\n",
    "\n",
    "        # Ensure that all the lists in the dictionary have the same length\n",
    "        num_rows = min(len(titles), len(dates), len(links), len(word))\n",
    "        new_data = {'Title': titles[:num_rows], 'Date': dates[:num_rows], 'URL': links[:num_rows], 'Description': word[:num_rows]}\n",
    "\n",
    "        # Convert the dictionary to a Pandas DataFrame\n",
    "        new_df = pd.DataFrame(new_data)\n",
    "\n",
    "        # Append the new DataFrame to the original DataFrame\n",
    "        df = pd.concat([df, new_df], ignore_index=True)\n",
    "\n",
    "        # Wait for the next button to become clickable\n",
    "        wait = WebDriverWait(driver, 10)\n",
    "        next_link = wait.until(EC.element_to_be_clickable((By.XPATH, \"//a[@class='next']\")))\n",
    "\n",
    "        # Check if the next link has a valid href attribute\n",
    "        if next_link.get_attribute('href') == \"javascript:;\":\n",
    "            # If the href attribute is \"javascript:;\", it means that we have reached the last page\n",
    "            break\n",
    "        else:\n",
    "            # Click on the next button\n",
    "            next_link.click()\n",
    "\n",
    "    except StaleElementReferenceException:\n",
    "        # If the element reference is stale, locate the element again\n",
    "        pass\n",
    "\n",
    "    except TimeoutException:\n",
    "        # If the \"next\" button is not clickable, it means that we have reached the last page\n",
    "        break\n",
    "        \n",
    "        \n",
    "\n",
    "# Close the driver\n",
    "driver.quit()\n",
    "\n",
    "df.drop_duplicates(inplace=True)\n",
    "\n",
    "df['Category'] = '拍客日记'\n",
    "\n",
    "df_pkrj = df\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d0d036c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    " # This is the fifth page \"文化印记 \"\n",
    "\n",
    "# Define the URL\n",
    "url = 'https://www.tsinghua.edu.cn/news/spqh/whyj.htm'\n",
    "\n",
    "driver = webdriver.Safari()\n",
    "\n",
    "# Open the web page in the driver\n",
    "driver.get(url)\n",
    "\n",
    "\n",
    "# Extract the HTML from the web page\n",
    "html = driver.page_source\n",
    "\n",
    "# Parse the HTML with Beautiful Soup\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# Define a list to store the extracted data\n",
    "df = pd.DataFrame(columns=['Title', 'Date', 'URL', 'Description'])\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        # Extract the HTML from the web page\n",
    "        html = driver.page_source\n",
    "\n",
    "        # Parse the HTML with Beautiful Soup\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "        # Extract the data from the soup object\n",
    "        video_title = soup.find_all('p', class_=\"bt\")[1:9]\n",
    "        titles = [vedio.text.strip() for vedio in video_title]\n",
    "\n",
    "        # Find the <ul> tag with class \"qhrw2_ul\"\n",
    "        ul_tag = soup.find('ul', class_='qhrw2_ul')\n",
    "        # Find all <a> tags within the <ul> tag and get their href attributes     \n",
    "        links = [urljoin(url, a_tag.get('href')) for a_tag in ul_tag.find_all('a')]\n",
    "\n",
    "        # Find all <div> tags with class \"sj\"\n",
    "        div_tags = ul_tag.find_all('div', class_='sj')\n",
    "        # Loop through each <div> tag and get the date\n",
    "        dates = []\n",
    "        for div_tag in div_tags:\n",
    "            p_tag = div_tag.find('p')\n",
    "            span_tag = div_tag.find('span')\n",
    "            date_str = f\"{p_tag.text}/{span_tag.text}\"\n",
    "            date = datetime.strptime(date_str, '%d/%Y.%m')\n",
    "            dates.append(date.strftime('%Y/%m/%d'))\n",
    "\n",
    "        # For the description    \n",
    "        words = soup.find_all('p', class_='zy')[1:]\n",
    "        word = [word.text.strip() for word in words]\n",
    "\n",
    "        # Ensure that all the lists in the dictionary have the same length\n",
    "        num_rows = min(len(titles), len(dates), len(links), len(word))\n",
    "        new_data = {'Title': titles[:num_rows], 'Date': dates[:num_rows], 'URL': links[:num_rows], 'Description': word[:num_rows]}\n",
    "\n",
    "        # Convert the dictionary to a Pandas DataFrame\n",
    "        new_df = pd.DataFrame(new_data)\n",
    "\n",
    "        # Append the new DataFrame to the original DataFrame\n",
    "        df = pd.concat([df, new_df], ignore_index=True)\n",
    "\n",
    "        # Wait for the next button to become clickable\n",
    "        wait = WebDriverWait(driver, 15)\n",
    "        next_link = wait.until(EC.element_to_be_clickable((By.XPATH, \"//a[@class='next']\")))\n",
    "\n",
    "        # Check if the next link has a valid href attribute\n",
    "        if next_link.get_attribute('href') == \"javascript:;\":\n",
    "            # If the href attribute is \"javascript:;\", it means that we have reached the last page\n",
    "            break\n",
    "        else:\n",
    "            # Click on the next button\n",
    "            next_link.click()\n",
    "\n",
    "    except StaleElementReferenceException:\n",
    "        # If the element reference is stale, locate the element again\n",
    "        pass\n",
    "\n",
    "    except TimeoutException:\n",
    "        # If the \"next\" button is not clickable, it means that we have reached the last page\n",
    "        break\n",
    "        \n",
    "        \n",
    "\n",
    "# Close the driver\n",
    "driver.quit()\n",
    "\n",
    "df.drop_duplicates(inplace=True)\n",
    "\n",
    "df['Category'] = '文化印记'\n",
    "\n",
    "df_whyj = df\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b44fe113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    " # This is the sixth page \"活动全录 \"\n",
    "\n",
    "# Define the URL\n",
    "url = 'https://www.tsinghua.edu.cn/news/spqh/hdql.htm'\n",
    "\n",
    "driver = webdriver.Safari()\n",
    "\n",
    "# Open the web page in the driver\n",
    "driver.get(url)\n",
    "\n",
    "\n",
    "# Extract the HTML from the web page\n",
    "html = driver.page_source\n",
    "\n",
    "# Parse the HTML with Beautiful Soup\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# Define a list to store the extracted data\n",
    "df = pd.DataFrame(columns=['Title', 'Date', 'URL', 'Description'])\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        # Extract the HTML from the web page\n",
    "        html = driver.page_source\n",
    "\n",
    "        # Parse the HTML with Beautiful Soup\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "        # Extract the data from the soup object\n",
    "        video_title = soup.find_all('p', class_=\"bt\")[1:9]\n",
    "        titles = [vedio.text.strip() for vedio in video_title]\n",
    "\n",
    "        # Find the <ul> tag with class \"qhrw2_ul\"\n",
    "        ul_tag = soup.find('ul', class_='qhrw2_ul')\n",
    "        # Find all <a> tags within the <ul> tag and get their href attributes     \n",
    "        links = [urljoin(url, a_tag.get('href')) for a_tag in ul_tag.find_all('a')]\n",
    "\n",
    "        # Find all <div> tags with class \"sj\"\n",
    "        div_tags = ul_tag.find_all('div', class_='sj')\n",
    "        # Loop through each <div> tag and get the date\n",
    "        dates = []\n",
    "        for div_tag in div_tags:\n",
    "            p_tag = div_tag.find('p')\n",
    "            span_tag = div_tag.find('span')\n",
    "            date_str = f\"{p_tag.text}/{span_tag.text}\"\n",
    "            date = datetime.strptime(date_str, '%d/%Y.%m')\n",
    "            dates.append(date.strftime('%Y/%m/%d'))\n",
    "\n",
    "        # For the description    \n",
    "        words = soup.find_all('p', class_='zy')[1:]\n",
    "        word = [word.text.strip() for word in words]\n",
    "\n",
    "        # Ensure that all the lists in the dictionary have the same length\n",
    "        num_rows = min(len(titles), len(dates), len(links), len(word))\n",
    "        new_data = {'Title': titles[:num_rows], 'Date': dates[:num_rows], 'URL': links[:num_rows], 'Description': word[:num_rows]}\n",
    "\n",
    "        # Convert the dictionary to a Pandas DataFrame\n",
    "        new_df = pd.DataFrame(new_data)\n",
    "\n",
    "        # Append the new DataFrame to the original DataFrame\n",
    "        df = pd.concat([df, new_df], ignore_index=True)\n",
    "\n",
    "        # Wait for the next button to become clickable\n",
    "        wait = WebDriverWait(driver, 10)\n",
    "        next_link = wait.until(EC.element_to_be_clickable((By.XPATH, \"//a[@class='next']\")))\n",
    "\n",
    "        # Check if the next link has a valid href attribute\n",
    "        if next_link.get_attribute('href') == \"javascript:;\":\n",
    "            # If the href attribute is \"javascript:;\", it means that we have reached the last page\n",
    "            break\n",
    "        else:\n",
    "            # Click on the next button\n",
    "            next_link.click()\n",
    "\n",
    "    except StaleElementReferenceException:\n",
    "        # If the element reference is stale, locate the element again\n",
    "        pass\n",
    "\n",
    "    except TimeoutException:\n",
    "        # If the \"next\" button is not clickable, it means that we have reached the last page\n",
    "        break\n",
    "        \n",
    "        \n",
    "\n",
    "# Close the driver\n",
    "driver.quit()\n",
    "\n",
    "df.drop_duplicates(inplace=True)\n",
    "\n",
    "df['Category'] = '活动全录'\n",
    "\n",
    "df_hdjl = df\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8f10a965",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined = pd.concat([df_jctj,df_qhyx,df_ztjj,df_pkrj,df_whyj, df_hdjl])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9519674d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the DataFrame to a CSV file\n",
    "df_combined.to_csv(r'/Users/davidsmacbook/Desktop/Work/THU_Website.csv', index=False)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "478398b2",
   "metadata": {},
   "source": [
    "# First Featured post\n",
    "video_title = soup.find_all('p', class_ =\"bt\")[0]\n",
    "titles = [vedio.text.strip() for vedio in video_title]\n",
    "titles"
   ]
  },
  {
   "cell_type": "raw",
   "id": "53aa7cce",
   "metadata": {},
   "source": [
    "# Find the <ul> tag with class \"qhrw2_first\"\n",
    "\n",
    "ul_tag = soup.find('div', class_='qhrw2_first')\n",
    "links = [a_tag.get('href') for a_tag in ul_tag.find_all('a')]\n",
    "\n",
    "links\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d3923586",
   "metadata": {},
   "source": [
    "# Find the <ul> tag with class \"qhrw2_ul\"\n",
    "ul_tag = soup.find('div', class_='sj')\n",
    "dates = []\n",
    "p_tag = ul_tag.find('p')\n",
    "span_tag = ul_tag.find('span')\n",
    "date_str = f\"{p_tag.text}/{span_tag.text}\"\n",
    "date = datetime.strptime(date_str, '%d/%Y.%m')\n",
    "dates.append(date.strftime('%Y/%m/%d'))\n",
    "\n",
    "\n",
    "dates\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cf84a81a",
   "metadata": {},
   "source": [
    "words = soup.find_all('p', class_='zy')[0]\n",
    "\n",
    "word = [word.text.strip() for word in words]\n",
    "\n",
    "word"
   ]
  },
  {
   "cell_type": "raw",
   "id": "73bf3fd6",
   "metadata": {},
   "source": [
    "video_title = soup.find_all('p', class_ =\"bt\")[1:9]\n",
    "titles = [vedio.text.strip() for vedio in video_title]\n",
    "\n",
    "titles\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cfecf221",
   "metadata": {},
   "source": [
    "# Find the <ul> tag with class \"qhrw2_ul\"\n",
    "ul_tag = soup.find('ul', class_='qhrw2_ul')\n",
    "\n",
    "# Find all <a> tags within the <ul> tag and get their href attributes\n",
    "links = [a_tag.get('href') for a_tag in ul_tag.find_all('a')]\n",
    "\n",
    "#links = [link.get('herf') for link in soup.find_all('ul', class_='qhrw2_ul')]\n",
    "\n",
    "links\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c7654596",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Find the <ul> tag with class \"qhrw2_ul\"\n",
    "ul_tag = soup.find('ul', class_='qhrw2_ul')\n",
    "\n",
    "# Find all <div> tags with class \"sj\"\n",
    "div_tags = ul_tag.find_all('div', class_='sj')\n",
    "\n",
    "# Loop through each <div> tag and get the date\n",
    "dates = []\n",
    "for div_tag in div_tags:\n",
    "    p_tag = div_tag.find('p')\n",
    "    span_tag = div_tag.find('span')\n",
    "    date_str = f\"{p_tag.text}/{span_tag.text}\"\n",
    "    date = datetime.strptime(date_str, '%d/%Y.%m')\n",
    "    dates.append(date.strftime('%Y/%m/%d'))\n",
    "\n",
    "# Print the list of dates\n",
    "print(dates)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "68fbb5dc",
   "metadata": {},
   "source": [
    "words = soup.find_all('p', class_='zy')[1:]\n",
    "\n",
    "word = [word.text.strip() for word in words]\n",
    "\n",
    "word"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9adb5d92",
   "metadata": {},
   "source": [
    "# Extract the data from the soup object\n",
    "video_title = soup.find_all('div', class_ =\"item-title04 item-title05\")\n",
    "titles = [vedio.text.strip() for vedio in video_title]\n",
    "\n",
    "dates = soup.find_all('span', class_ =\"item-date\")[3:]\n",
    "date = [vedio.text.strip() for vedio in dates]\n",
    "\n",
    "links = soup.find_all('source')\n",
    "link = [vediolink['src'] for vediolink in links][3:]\n",
    "\n",
    "Words =\n",
    "word"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
